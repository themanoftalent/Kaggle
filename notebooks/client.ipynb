{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods for Optimal Machine Learning Results\n",
    "\n",
    "This notebook demonstrates how to use ensemble methods to achieve high-performance results in a classification task. We’ll use a synthetic dataset, preprocess it, and apply multiple ensemble techniques including Random Forest, Gradient Boosting, and a Voting Classifier. Each method will be evaluated, and we’ll compare their performance.\n",
    "\n",
    "## Objectives\n",
    "- Generate a sample dataset\n",
    "- Preprocess the data\n",
    "- Train and evaluate individual ensemble models\n",
    "- Combine models using a Voting Classifier\n",
    "- Compare results\n",
    "\n",
    "## Libraries Used\n",
    "- `numpy` and `pandas` for data manipulation\n",
    "- `sklearn` for machine learning models and metrics\n",
    "- `matplotlib` and `seaborn` for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Dataset\n",
    "\n",
    "We’ll create a synthetic classification dataset with 1000 samples, 20 features, and 2 classes using `make_classification`. This simulates a real-world scenario where we have features and a target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Individual Ensemble Models\n",
    "\n",
    "We’ll train two powerful ensemble models: Random Forest and Gradient Boosting. These models aggregate predictions from multiple decision trees to improve accuracy and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Voting Classifier (Ensemble of Models)\n",
    "\n",
    "To combine the strengths of different models, we’ll use a Voting Classifier that integrates Random Forest, Gradient Boosting, and Logistic Regression. We’ll use soft voting, which averages predicted probabilities for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(random_state=42)\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('rf', rf_model),\n",
    "    ('gb', gb_model),\n",
    "    ('lr', lr_model)\n",
    "], voting='soft')\n",
    "voting_model.fit(X_train, y_train)\n",
    "voting_pred = voting_model.predict(X_test)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluation and Comparison\n",
    "\n",
    "We’ll evaluate each model using accuracy and a detailed classification report (precision, recall, F1-score). Then, we’ll visualize the results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Gradient Boosting Accuracy:\", gb_accuracy)\n",
    "print(\"Voting Classifier Accuracy:\", voting_accuracy)\n",
    "\n",
    "print(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, rf_pred))\n",
    "print(\"Gradient Boosting Classification Report:\\n\", classification_report(y_test, gb_pred))\n",
    "print(\"Voting Classifier Classification Report:\\n\", classification_report(y_test, voting_pred))\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosting', 'Voting Classifier'],\n",
    "    'Accuracy': [rf_accuracy, gb_accuracy, voting_accuracy]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Model', y='Accuracy', data=results, palette='viridis')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook implemented and compared three ensemble approaches:\n",
    "- **Random Forest**: A bagging method that reduces variance.\n",
    "- **Gradient Boosting**: A boosting method that reduces bias.\n",
    "- **Voting Classifier**: A hybrid approach combining multiple models.\n",
    "\n",
    "The Voting Classifier often achieves the best results by leveraging the strengths of all models. You can adapt this code to your specific dataset by replacing the synthetic data generation with your own data loading step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
